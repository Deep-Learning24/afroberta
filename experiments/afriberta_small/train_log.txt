2024-12-28 13:38:22,423 Experiment Output Path: experiments/afriberta_small
2024-12-28 13:38:22,423 Training will be done with this configuration: 
 {'model': {'tokenizer_path': 'afriberta_tokenizer_70k', 'layer_norm_eps': 1e-05, 'output_past': True, 'type_vocab_size': 1, 'max_length': 512, 'hidden_size': 768, 'num_attention_heads': 6, 'num_hidden_layers': 4, 'intermediate_size': 3072}, 'training': {'gradient_accumulation_steps': 8, 'resume_training': False, 'ignore_data_skip': False, 'train_from_scratch': True, 'use_whole_word_mask': False, 'lang_sampling_factor': 1.0, 'overwrite_output_dir': False, 'seed': 999, 'max_steps': 460000, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'dataloader_num_workers': 6, 'fp16': True, 'save_steps': 20000, 'save_total_limit': 10, 'learning_rate': 0.0001, 'warmup_steps': 40000, 'output_dir': 'experiments/afriberta_small'}, 'data': {'train': 'data/train/', 'eval': {'all': 'data/eval/all_eval.txt', 'per_lang': 'data/eval/'}}} 
2024-12-28 13:38:22,424 Training from scratch...
2024-12-29 17:05:43,793 Experiment Output Path: experiments/afriberta_small
2024-12-29 17:05:43,794 Training will be done with this configuration: 
 {'model': {'tokenizer_path': 'afriberta_tokenizer_70k', 'layer_norm_eps': 1e-05, 'output_past': True, 'type_vocab_size': 1, 'max_length': 512, 'hidden_size': 768, 'num_attention_heads': 6, 'num_hidden_layers': 4, 'intermediate_size': 3072}, 'training': {'gradient_accumulation_steps': 8, 'resume_training': False, 'ignore_data_skip': False, 'train_from_scratch': True, 'use_whole_word_mask': False, 'lang_sampling_factor': 1.0, 'overwrite_output_dir': False, 'seed': 999, 'max_steps': 460000, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'dataloader_num_workers': 6, 'fp16': True, 'save_steps': 20000, 'save_total_limit': 10, 'learning_rate': 0.0001, 'warmup_steps': 40000, 'output_dir': 'experiments/afriberta_small'}, 'data': {'train': 'data/train/', 'eval': {'all': 'data/eval/all_eval.txt', 'per_lang': 'data/eval/'}}} 
2024-12-29 17:05:43,794 Training from scratch...
2024-12-29 17:16:07,505 Experiment Output Path: experiments/afriberta_small
2024-12-29 17:16:07,506 Training will be done with this configuration: 
 {'model': {'tokenizer_path': 'afriberta_tokenizer_70k', 'layer_norm_eps': 1e-05, 'output_past': True, 'type_vocab_size': 1, 'max_length': 512, 'hidden_size': 768, 'num_attention_heads': 6, 'num_hidden_layers': 4, 'intermediate_size': 3072}, 'training': {'gradient_accumulation_steps': 8, 'resume_training': False, 'ignore_data_skip': False, 'train_from_scratch': True, 'use_whole_word_mask': False, 'lang_sampling_factor': 1.0, 'overwrite_output_dir': False, 'seed': 999, 'max_steps': 460000, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'dataloader_num_workers': 6, 'fp16': True, 'save_steps': 20000, 'save_total_limit': 10, 'learning_rate': 0.0001, 'warmup_steps': 40000, 'output_dir': 'experiments/afriberta_small'}, 'data': {'train': 'data/train/', 'eval': {'all': 'data/eval/all_eval.txt', 'per_lang': 'data/eval/'}}} 
2024-12-29 17:16:07,506 Training from scratch...
2024-12-29 17:18:18,795 Experiment Output Path: experiments/afriberta_small
2024-12-29 17:18:18,796 Training will be done with this configuration: 
 {'model': {'tokenizer_path': 'afriberta_tokenizer_70k', 'layer_norm_eps': 1e-05, 'output_past': True, 'type_vocab_size': 1, 'max_length': 512, 'hidden_size': 768, 'num_attention_heads': 6, 'num_hidden_layers': 4, 'intermediate_size': 3072}, 'training': {'gradient_accumulation_steps': 8, 'resume_training': False, 'ignore_data_skip': False, 'train_from_scratch': True, 'use_whole_word_mask': False, 'lang_sampling_factor': 1.0, 'overwrite_output_dir': False, 'seed': 999, 'max_steps': 460000, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'dataloader_num_workers': 6, 'fp16': True, 'save_steps': 20000, 'save_total_limit': 10, 'learning_rate': 0.0001, 'warmup_steps': 40000, 'output_dir': 'experiments/afriberta_small'}, 'data': {'train': 'data/train/', 'eval': {'all': 'data/eval/all_eval.txt', 'per_lang': 'data/eval/'}}} 
2024-12-29 17:18:18,796 Training from scratch...
2024-12-29 17:27:44,107 Experiment Output Path: experiments/afriberta_small
2024-12-29 17:27:44,108 Training will be done with this configuration: 
 {'model': {'tokenizer_path': 'castorini/afriberta_small', 'layer_norm_eps': 1e-05, 'output_past': True, 'type_vocab_size': 1, 'max_length': 512, 'hidden_size': 768, 'num_attention_heads': 6, 'num_hidden_layers': 4, 'intermediate_size': 3072}, 'training': {'gradient_accumulation_steps': 8, 'resume_training': False, 'ignore_data_skip': False, 'train_from_scratch': True, 'use_whole_word_mask': False, 'lang_sampling_factor': 1.0, 'overwrite_output_dir': False, 'seed': 999, 'max_steps': 460000, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'dataloader_num_workers': 6, 'fp16': True, 'save_steps': 20000, 'save_total_limit': 10, 'learning_rate': 0.0001, 'warmup_steps': 40000, 'output_dir': 'experiments/afriberta_small'}, 'data': {'train': 'data/train/', 'eval': {'all': 'data/eval/all_eval.txt', 'per_lang': 'data/eval/'}}} 
2024-12-29 17:27:44,108 Training from scratch...
2024-12-29 17:27:50,180 Building model...
2024-12-29 17:27:52,326 Model built with num parameters: 83175286
2024-12-29 17:27:52,326 Building datasets...
2024-12-29 17:27:52,326 Building train dataset from data/train/...
2024-12-29 17:27:52,329 No. of training sentences: 0
2024-12-29 17:27:52,329 Building evaluation dataset from data/eval/all_eval.txt...
2024-12-29 17:43:25,094 Experiment Output Path: experiments/afriberta_small
2024-12-29 17:43:25,095 Training will be done with this configuration: 
 {'model': {'tokenizer_path': 'castorini/afriberta_small', 'layer_norm_eps': 1e-05, 'output_past': True, 'type_vocab_size': 1, 'max_length': 512, 'hidden_size': 768, 'num_attention_heads': 6, 'num_hidden_layers': 4, 'intermediate_size': 3072}, 'training': {'gradient_accumulation_steps': 8, 'resume_training': False, 'ignore_data_skip': False, 'train_from_scratch': True, 'use_whole_word_mask': False, 'lang_sampling_factor': 1.0, 'overwrite_output_dir': False, 'seed': 999, 'max_steps': 460000, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'dataloader_num_workers': 6, 'fp16': True, 'save_steps': 20000, 'save_total_limit': 10, 'learning_rate': 0.0001, 'warmup_steps': 40000, 'output_dir': 'experiments/afriberta_small'}, 'data': {'data_source': 'local', 'all_data': 'data/newsites.txt', 'train': 'data/train/', 'eval': {'all': 'data/eval/all_eval.txt', 'per_lang': 'data/eval/'}}} 
2024-12-29 17:43:25,095 Training from scratch...
2024-12-29 17:43:26,035 Building model...
2024-12-29 17:43:28,387 Model built with num parameters: 83175286
2024-12-29 17:43:28,387 Building datasets...
2024-12-29 17:43:28,387 Building train dataset from data/train/...
2024-12-29 17:44:55,921 Experiment Output Path: experiments/afriberta_small
2024-12-29 17:44:55,922 Training will be done with this configuration: 
 {'model': {'tokenizer_path': 'castorini/afriberta_small', 'layer_norm_eps': 1e-05, 'output_past': True, 'type_vocab_size': 1, 'max_length': 512, 'hidden_size': 768, 'num_attention_heads': 6, 'num_hidden_layers': 4, 'intermediate_size': 3072}, 'training': {'gradient_accumulation_steps': 8, 'resume_training': False, 'ignore_data_skip': False, 'train_from_scratch': True, 'use_whole_word_mask': False, 'lang_sampling_factor': 1.0, 'overwrite_output_dir': False, 'seed': 999, 'max_steps': 460000, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'dataloader_num_workers': 6, 'fp16': True, 'save_steps': 20000, 'save_total_limit': 10, 'learning_rate': 0.0001, 'warmup_steps': 40000, 'output_dir': 'experiments/afriberta_small'}, 'data': {'data_source': 'local', 'all_data': 'data/newsites.txt', 'train': 'data/train/', 'eval': {'all': 'data/eval/all_eval.txt', 'per_lang': 'data/eval/'}}} 
2024-12-29 17:44:55,922 Training from scratch...
2024-12-29 17:44:56,871 Building model...
2024-12-29 17:44:59,039 Model built with num parameters: 83175286
2024-12-29 17:44:59,040 Building datasets...
2024-12-29 17:44:59,040 Building train dataset from data/train/...
2024-12-29 17:44:59,047 No. of training sentences: 0
2024-12-29 17:44:59,048 Building evaluation dataset from data/eval/all_eval.txt...
2024-12-29 17:48:10,615 Experiment Output Path: experiments/afriberta_small
2024-12-29 17:48:10,616 Training will be done with this configuration: 
 {'model': {'tokenizer_path': 'castorini/afriberta_small', 'layer_norm_eps': 1e-05, 'output_past': True, 'type_vocab_size': 1, 'max_length': 512, 'hidden_size': 768, 'num_attention_heads': 6, 'num_hidden_layers': 4, 'intermediate_size': 3072}, 'training': {'gradient_accumulation_steps': 8, 'resume_training': False, 'ignore_data_skip': False, 'train_from_scratch': True, 'use_whole_word_mask': False, 'lang_sampling_factor': 1.0, 'overwrite_output_dir': False, 'seed': 999, 'max_steps': 460000, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'dataloader_num_workers': 6, 'fp16': True, 'save_steps': 20000, 'save_total_limit': 10, 'learning_rate': 0.0001, 'warmup_steps': 40000, 'output_dir': 'experiments/afriberta_small'}, 'data': {'data_source': 'local', 'all_data': 'data/newsites.txt', 'train': 'data/train/', 'eval': {'all': 'data/eval/all_eval.txt', 'per_lang': 'data/eval/'}}} 
2024-12-29 17:48:10,616 Training from scratch...
2024-12-29 17:48:11,931 Building model...
2024-12-29 17:48:14,582 Model built with num parameters: 83175286
2024-12-29 17:48:14,583 Building datasets...
2024-12-29 17:48:14,583 Building train dataset from data/train/...
2024-12-29 17:48:14,587 No. of training sentences: 0
2024-12-29 17:48:14,587 Building evaluation dataset from data/eval/all_eval.txt...
2024-12-29 17:51:39,519 Experiment Output Path: experiments/afriberta_small
2024-12-29 17:51:39,519 Training will be done with this configuration: 
 {'model': {'tokenizer_path': 'castorini/afriberta_small', 'layer_norm_eps': 1e-05, 'output_past': True, 'type_vocab_size': 1, 'max_length': 512, 'hidden_size': 768, 'num_attention_heads': 6, 'num_hidden_layers': 4, 'intermediate_size': 3072}, 'training': {'gradient_accumulation_steps': 8, 'resume_training': False, 'ignore_data_skip': False, 'train_from_scratch': True, 'use_whole_word_mask': False, 'lang_sampling_factor': 1.0, 'overwrite_output_dir': False, 'seed': 999, 'max_steps': 460000, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'dataloader_num_workers': 6, 'fp16': True, 'save_steps': 20000, 'save_total_limit': 10, 'learning_rate': 0.0001, 'warmup_steps': 40000, 'output_dir': 'experiments/afriberta_small'}, 'data': {'data_source': 'local', 'all_data': 'data/newsites.txt', 'train': 'data/train/', 'eval': {'all': 'data/eval/all_eval.txt', 'per_lang': 'data/eval/'}}} 
2024-12-29 17:51:39,520 Training from scratch...
2024-12-29 17:51:40,389 Building model...
2024-12-29 17:51:42,578 Model built with num parameters: 83175286
2024-12-29 17:51:42,578 Building datasets...
2024-12-29 17:51:42,579 Building train dataset from data/train/...
2024-12-29 17:52:35,126 Experiment Output Path: experiments/afriberta_small
2024-12-29 17:52:35,126 Training will be done with this configuration: 
 {'model': {'tokenizer_path': 'castorini/afriberta_small', 'layer_norm_eps': 1e-05, 'output_past': True, 'type_vocab_size': 1, 'max_length': 512, 'hidden_size': 768, 'num_attention_heads': 6, 'num_hidden_layers': 4, 'intermediate_size': 3072}, 'training': {'gradient_accumulation_steps': 8, 'resume_training': False, 'ignore_data_skip': False, 'train_from_scratch': True, 'use_whole_word_mask': False, 'lang_sampling_factor': 1.0, 'overwrite_output_dir': False, 'seed': 999, 'max_steps': 460000, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'dataloader_num_workers': 6, 'fp16': True, 'save_steps': 20000, 'save_total_limit': 10, 'learning_rate': 0.0001, 'warmup_steps': 40000, 'output_dir': 'experiments/afriberta_small'}, 'data': {'data_source': 'local', 'all_data': 'data/newsites.txt', 'train': 'data/train/', 'eval': {'all': 'data/eval/all_eval.txt', 'per_lang': 'data/eval/'}}} 
2024-12-29 17:52:35,126 Training from scratch...
2024-12-29 17:52:35,955 Building model...
2024-12-29 17:52:38,187 Model built with num parameters: 83175286
2024-12-29 17:52:38,187 Building datasets...
2024-12-29 17:52:38,187 Building train dataset from data/train/...
2024-12-29 17:52:41,981 No. of training sentences: 0
2024-12-29 17:52:41,982 Building evaluation dataset from data/eval/all_eval.txt...
2024-12-29 17:57:53,255 Experiment Output Path: experiments/afriberta_small
2024-12-29 17:57:53,255 Training will be done with this configuration: 
 {'model': {'tokenizer_path': 'castorini/afriberta_small', 'layer_norm_eps': 1e-05, 'output_past': True, 'type_vocab_size': 1, 'max_length': 512, 'hidden_size': 768, 'num_attention_heads': 6, 'num_hidden_layers': 4, 'intermediate_size': 3072}, 'training': {'gradient_accumulation_steps': 8, 'resume_training': False, 'ignore_data_skip': False, 'train_from_scratch': True, 'use_whole_word_mask': False, 'lang_sampling_factor': 1.0, 'overwrite_output_dir': False, 'seed': 999, 'max_steps': 460000, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'dataloader_num_workers': 6, 'fp16': True, 'save_steps': 20000, 'save_total_limit': 10, 'learning_rate': 0.0001, 'warmup_steps': 40000, 'output_dir': 'experiments/afriberta_small'}, 'data': {'data_source': 'local', 'all_data': 'data/newsites.txt', 'train': 'data/train/', 'eval': {'all': 'data/eval/all_eval.txt', 'per_lang': 'data/eval/'}}} 
2024-12-29 17:57:53,256 Training from scratch...
2024-12-29 17:57:54,154 Building model...
2024-12-29 17:57:56,251 Model built with num parameters: 83175286
2024-12-29 17:57:56,251 Building datasets...
2024-12-29 17:57:56,252 Building train dataset from data/train/...
2024-12-29 18:01:26,711 Experiment Output Path: experiments/afriberta_small
2024-12-29 18:01:26,712 Training will be done with this configuration: 
 {'model': {'tokenizer_path': 'castorini/afriberta_small', 'layer_norm_eps': 1e-05, 'output_past': True, 'type_vocab_size': 1, 'max_length': 512, 'hidden_size': 768, 'num_attention_heads': 6, 'num_hidden_layers': 4, 'intermediate_size': 3072}, 'training': {'gradient_accumulation_steps': 8, 'resume_training': False, 'ignore_data_skip': False, 'train_from_scratch': True, 'use_whole_word_mask': False, 'lang_sampling_factor': 1.0, 'overwrite_output_dir': False, 'seed': 999, 'max_steps': 460000, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'dataloader_num_workers': 6, 'fp16': True, 'save_steps': 20000, 'save_total_limit': 10, 'learning_rate': 0.0001, 'warmup_steps': 40000, 'output_dir': 'experiments/afriberta_small'}, 'data': {'data_source': 'local', 'all_data': 'data/newsites.txt', 'train': 'data/train/', 'eval': {'all': 'data/eval/all_eval.txt', 'per_lang': 'data/eval/'}}} 
2024-12-29 18:01:26,712 Training from scratch...
2024-12-29 18:01:27,664 Building model...
2024-12-29 18:01:30,029 Model built with num parameters: 83175286
2024-12-29 18:01:30,030 Building datasets...
2024-12-29 18:01:30,030 Building train dataset from data/train/...
2024-12-29 18:05:10,283 Experiment Output Path: experiments/afriberta_small
2024-12-29 18:05:10,284 Training will be done with this configuration: 
 {'model': {'tokenizer_path': 'castorini/afriberta_small', 'layer_norm_eps': 1e-05, 'output_past': True, 'type_vocab_size': 1, 'max_length': 512, 'hidden_size': 768, 'num_attention_heads': 6, 'num_hidden_layers': 4, 'intermediate_size': 3072}, 'training': {'gradient_accumulation_steps': 8, 'resume_training': False, 'ignore_data_skip': False, 'train_from_scratch': True, 'use_whole_word_mask': False, 'lang_sampling_factor': 1.0, 'overwrite_output_dir': False, 'seed': 999, 'max_steps': 460000, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'dataloader_num_workers': 6, 'fp16': True, 'save_steps': 20000, 'save_total_limit': 10, 'learning_rate': 0.0001, 'warmup_steps': 40000, 'output_dir': 'experiments/afriberta_small'}, 'data': {'data_source': 'local', 'all_data': 'data/newsites.txt', 'train': 'data/train/', 'eval': {'all': 'data/eval/all_eval.txt', 'per_lang': 'data/eval/'}}} 
2024-12-29 18:05:10,284 Training from scratch...
2024-12-29 18:05:11,157 Building model...
2024-12-29 18:05:14,074 Model built with num parameters: 83175286
2024-12-29 18:05:14,080 Building datasets...
2024-12-29 18:05:14,081 Building train dataset from data/train/...
2024-12-29 18:08:42,710 Experiment Output Path: experiments/afriberta_small
2024-12-29 18:08:42,711 Training will be done with this configuration: 
 {'model': {'tokenizer_path': 'castorini/afriberta_small', 'layer_norm_eps': 1e-05, 'output_past': True, 'type_vocab_size': 1, 'max_length': 512, 'hidden_size': 768, 'num_attention_heads': 6, 'num_hidden_layers': 4, 'intermediate_size': 3072}, 'training': {'gradient_accumulation_steps': 8, 'resume_training': False, 'ignore_data_skip': False, 'train_from_scratch': True, 'use_whole_word_mask': False, 'lang_sampling_factor': 1.0, 'overwrite_output_dir': False, 'seed': 999, 'max_steps': 460000, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'dataloader_num_workers': 6, 'fp16': True, 'save_steps': 20000, 'save_total_limit': 10, 'learning_rate': 0.0001, 'warmup_steps': 40000, 'output_dir': 'experiments/afriberta_small'}, 'data': {'data_source': 'local', 'all_data': 'data/newsites.txt', 'train': 'data/train/', 'eval': {'all': 'data/eval/all_eval.txt', 'per_lang': 'data/eval/'}}} 
2024-12-29 18:08:42,711 Training from scratch...
2024-12-29 18:08:43,570 Building model...
2024-12-29 18:08:45,677 Model built with num parameters: 83175286
2024-12-29 18:08:45,678 Building datasets...
2024-12-29 18:08:45,678 Building train dataset from data/train/...
2024-12-29 18:10:50,121 Experiment Output Path: experiments/afriberta_small
2024-12-29 18:10:50,122 Training will be done with this configuration: 
 {'model': {'tokenizer_path': 'castorini/afriberta_small', 'layer_norm_eps': 1e-05, 'output_past': True, 'type_vocab_size': 1, 'max_length': 512, 'hidden_size': 768, 'num_attention_heads': 6, 'num_hidden_layers': 4, 'intermediate_size': 3072}, 'training': {'gradient_accumulation_steps': 8, 'resume_training': False, 'ignore_data_skip': False, 'train_from_scratch': True, 'use_whole_word_mask': False, 'lang_sampling_factor': 1.0, 'overwrite_output_dir': False, 'seed': 999, 'max_steps': 460000, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'dataloader_num_workers': 6, 'fp16': True, 'save_steps': 20000, 'save_total_limit': 10, 'learning_rate': 0.0001, 'warmup_steps': 40000, 'output_dir': 'experiments/afriberta_small'}, 'data': {'data_source': 'local', 'all_data': 'data/newsites.txt', 'train': 'data/train/', 'eval': {'all': 'data/eval/all_eval.txt', 'per_lang': 'data/eval/'}}} 
2024-12-29 18:10:50,122 Training from scratch...
2024-12-29 18:10:50,995 Building model...
2024-12-29 18:10:53,328 Model built with num parameters: 83175286
2024-12-29 18:10:53,329 Building datasets...
2024-12-29 18:10:53,329 Building train dataset from data/train/...
2024-12-29 18:10:56,498 No. of training sentences: 0
2024-12-29 18:10:56,500 Building evaluation dataset from data/eval/all_eval.txt...
2024-12-29 18:13:26,495 Experiment Output Path: experiments/afriberta_small
2024-12-29 18:13:26,496 Training will be done with this configuration: 
 {'model': {'tokenizer_path': 'castorini/afriberta_small', 'layer_norm_eps': 1e-05, 'output_past': True, 'type_vocab_size': 1, 'max_length': 512, 'hidden_size': 768, 'num_attention_heads': 6, 'num_hidden_layers': 4, 'intermediate_size': 3072}, 'training': {'gradient_accumulation_steps': 8, 'resume_training': False, 'ignore_data_skip': False, 'train_from_scratch': True, 'use_whole_word_mask': False, 'lang_sampling_factor': 1.0, 'overwrite_output_dir': False, 'seed': 999, 'max_steps': 460000, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'dataloader_num_workers': 6, 'fp16': True, 'save_steps': 20000, 'save_total_limit': 10, 'learning_rate': 0.0001, 'warmup_steps': 40000, 'output_dir': 'experiments/afriberta_small'}, 'data': {'data_source': 'local', 'all_data': 'data/newsites.txt', 'train': 'data/train/', 'eval': {'all': 'data/eval/all_eval.txt', 'per_lang': 'data/eval/'}}} 
2024-12-29 18:13:26,497 Training from scratch...
2024-12-29 18:13:27,464 Building model...
2024-12-29 18:13:29,583 Model built with num parameters: 83175286
2024-12-29 18:13:29,583 Building datasets...
2024-12-29 18:13:29,583 Building train dataset from data/train/...
2024-12-29 18:13:34,298 No. of training sentences: 0
2024-12-29 18:13:34,299 Building evaluation dataset from data/eval/all_eval.txt...
2024-12-29 18:22:56,909 Experiment Output Path: experiments/afriberta_small
2024-12-29 18:22:56,910 Training will be done with this configuration: 
 {'model': {'tokenizer_path': 'castorini/afriberta_small', 'layer_norm_eps': 1e-05, 'output_past': True, 'type_vocab_size': 1, 'max_length': 512, 'hidden_size': 768, 'num_attention_heads': 6, 'num_hidden_layers': 4, 'intermediate_size': 3072}, 'training': {'gradient_accumulation_steps': 8, 'resume_training': False, 'ignore_data_skip': False, 'train_from_scratch': True, 'use_whole_word_mask': False, 'lang_sampling_factor': 1.0, 'overwrite_output_dir': False, 'seed': 999, 'max_steps': 460000, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'dataloader_num_workers': 6, 'fp16': True, 'save_steps': 20000, 'save_total_limit': 10, 'learning_rate': 0.0001, 'warmup_steps': 40000, 'output_dir': 'experiments/afriberta_small'}, 'data': {'data_source': 'local', 'all_data': 'data/newsites.txt', 'train': 'data/train/', 'eval': {'all': 'data/eval/all_eval.txt', 'per_lang': 'data/eval/'}}} 
2024-12-29 18:22:56,910 Training from scratch...
2024-12-29 18:22:57,801 Building model...
2024-12-29 18:23:00,053 Model built with num parameters: 83175286
2024-12-29 18:23:00,053 Building datasets...
2024-12-29 18:23:00,053 Building train dataset from data/train/...
2024-12-29 18:23:05,693 No. of training sentences: 0
2024-12-29 18:23:05,694 Building evaluation dataset from data/eval/all_eval.txt...
2024-12-29 18:25:14,873 Experiment Output Path: experiments/afriberta_small
2024-12-29 18:25:14,873 Training will be done with this configuration: 
 {'model': {'tokenizer_path': 'castorini/afriberta_small', 'layer_norm_eps': 1e-05, 'output_past': True, 'type_vocab_size': 1, 'max_length': 512, 'hidden_size': 768, 'num_attention_heads': 6, 'num_hidden_layers': 4, 'intermediate_size': 3072}, 'training': {'gradient_accumulation_steps': 8, 'resume_training': False, 'ignore_data_skip': False, 'train_from_scratch': True, 'use_whole_word_mask': False, 'lang_sampling_factor': 1.0, 'overwrite_output_dir': False, 'seed': 999, 'max_steps': 460000, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'dataloader_num_workers': 6, 'fp16': True, 'save_steps': 20000, 'save_total_limit': 10, 'learning_rate': 0.0001, 'warmup_steps': 40000, 'output_dir': 'experiments/afriberta_small'}, 'data': {'data_source': 'local', 'all_data': 'data/newsites.txt', 'train': 'data/train/', 'eval': {'all': 'data/eval/all_eval.txt', 'per_lang': 'data/eval/'}}} 
2024-12-29 18:25:14,873 Training from scratch...
2024-12-29 18:25:15,723 Building model...
2024-12-29 18:25:17,847 Model built with num parameters: 83175286
2024-12-29 18:25:17,847 Building datasets...
2024-12-29 18:25:17,847 Building train dataset from data/train/...
2024-12-29 18:25:25,507 No. of training sentences: 0
2024-12-29 18:25:25,510 Building evaluation dataset from data/eval/all_eval.txt...
2024-12-29 18:25:45,648 No. of evaluation sentences: 5954
2024-12-29 18:25:45,648 Starting Training...
2024-12-29 18:29:24,444 Experiment Output Path: experiments/afriberta_small
2024-12-29 18:29:24,445 Training will be done with this configuration: 
 {'model': {'tokenizer_path': 'castorini/afriberta_small', 'layer_norm_eps': 1e-05, 'output_past': True, 'type_vocab_size': 1, 'max_length': 512, 'hidden_size': 768, 'num_attention_heads': 6, 'num_hidden_layers': 4, 'intermediate_size': 3072}, 'training': {'gradient_accumulation_steps': 8, 'resume_training': False, 'ignore_data_skip': False, 'train_from_scratch': True, 'use_whole_word_mask': False, 'lang_sampling_factor': 1.0, 'overwrite_output_dir': False, 'seed': 999, 'max_steps': 460000, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'dataloader_num_workers': 6, 'fp16': True, 'save_steps': 20000, 'save_total_limit': 10, 'learning_rate': 0.0001, 'warmup_steps': 40000, 'output_dir': 'experiments/afriberta_small'}, 'data': {'all_data': 'data/newsites.txt', 'train': 'data/train/', 'eval': {'all': 'data/eval/all_eval.txt', 'per_lang': 'data/eval/'}}} 
2024-12-29 18:29:24,445 Training from scratch...
2024-12-29 18:29:25,313 Building model...
2024-12-29 18:29:27,374 Model built with num parameters: 83175286
2024-12-29 18:29:27,374 Building datasets...
2024-12-29 18:29:27,374 Building train dataset from data/train/...
2024-12-29 18:29:33,482 No. of training sentences: 0
2024-12-29 18:29:33,483 Building evaluation dataset from data/eval/all_eval.txt...
2024-12-29 18:29:52,869 No. of evaluation sentences: 5954
2024-12-29 18:29:52,869 Starting Training...
2024-12-29 18:36:31,096 Experiment Output Path: experiments/afriberta_small
2024-12-29 18:36:31,097 Training will be done with this configuration: 
 {'model': {'tokenizer_path': 'castorini/afriberta_small', 'layer_norm_eps': 1e-05, 'output_past': True, 'type_vocab_size': 1, 'max_length': 512, 'hidden_size': 768, 'num_attention_heads': 6, 'num_hidden_layers': 4, 'intermediate_size': 3072}, 'training': {'gradient_accumulation_steps': 8, 'resume_training': False, 'ignore_data_skip': False, 'train_from_scratch': True, 'use_whole_word_mask': False, 'lang_sampling_factor': 1.0, 'overwrite_output_dir': False, 'seed': 999, 'max_steps': 460000, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'dataloader_num_workers': 6, 'fp16': True, 'save_steps': 20000, 'save_total_limit': 10, 'learning_rate': 0.0001, 'warmup_steps': 40000, 'output_dir': 'experiments/afriberta_small'}, 'data': {'all_data': 'data/newsites.txt', 'train': 'data/train/', 'eval': {'all': 'data/eval/all_eval.txt', 'per_lang': 'data/eval/'}}} 
2024-12-29 18:36:31,097 Training from scratch...
2024-12-29 18:36:32,018 Building model...
2024-12-29 18:36:34,706 Model built with num parameters: 83175286
2024-12-29 18:36:34,706 Building datasets...
2024-12-29 18:36:34,707 Building train dataset from data/train/...
2024-12-29 18:36:40,466 No. of training sentences: 0
2024-12-29 18:36:40,467 Building evaluation dataset from data/eval/all_eval.txt...
2024-12-29 18:37:02,203 No. of evaluation sentences: 5954
2024-12-29 18:37:02,203 Starting Training...
2024-12-29 18:44:23,397 Experiment Output Path: experiments/afriberta_small
2024-12-29 18:44:23,397 Training will be done with this configuration: 
 {'model': {'tokenizer_path': 'castorini/afriberta_small', 'layer_norm_eps': 1e-05, 'output_past': True, 'type_vocab_size': 1, 'max_length': 512, 'hidden_size': 768, 'num_attention_heads': 6, 'num_hidden_layers': 4, 'intermediate_size': 3072}, 'training': {'gradient_accumulation_steps': 8, 'resume_training': False, 'ignore_data_skip': False, 'train_from_scratch': True, 'use_whole_word_mask': False, 'lang_sampling_factor': 1.0, 'overwrite_output_dir': False, 'seed': 999, 'max_steps': 460000, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'dataloader_num_workers': 6, 'fp16': True, 'save_steps': 20000, 'save_total_limit': 10, 'learning_rate': 0.0001, 'warmup_steps': 40000, 'output_dir': 'experiments/afriberta_small'}, 'data': {'all_data': 'data/newsites.txt', 'train': 'data/train/', 'eval': {'all': 'data/eval/all_eval.txt', 'per_lang': 'data/eval/'}}} 
2024-12-29 18:44:23,397 Training from scratch...
2024-12-29 18:44:24,407 Building model...
2024-12-29 18:44:27,699 Model built with num parameters: 83175286
2024-12-29 18:44:27,699 Building datasets...
2024-12-29 18:44:27,699 Building train dataset from data/train/...
2024-12-29 18:47:53,396 No. of training sentences: 47632
2024-12-29 18:47:53,401 Building evaluation dataset from data/eval/all_eval.txt...
2024-12-29 18:48:14,089 No. of evaluation sentences: 5954
2024-12-29 18:48:14,090 Starting Training...
2024-12-29 22:17:45,255 Experiment Output Path: experiments/afriberta_small
2024-12-29 22:17:45,255 Training will be done with this configuration: 
 {'model': {'tokenizer_path': 'castorini/afriberta_small', 'layer_norm_eps': 1e-05, 'output_past': True, 'type_vocab_size': 1, 'max_length': 512, 'hidden_size': 768, 'num_attention_heads': 6, 'num_hidden_layers': 4, 'intermediate_size': 3072}, 'training': {'gradient_accumulation_steps': 8, 'resume_training': False, 'ignore_data_skip': False, 'train_from_scratch': True, 'use_whole_word_mask': False, 'lang_sampling_factor': 1.0, 'overwrite_output_dir': False, 'seed': 999, 'max_steps': 460000, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'dataloader_num_workers': 6, 'fp16': True, 'save_steps': 20000, 'save_total_limit': 10, 'learning_rate': 0.0001, 'warmup_steps': 40000, 'output_dir': 'experiments/afriberta_small'}, 'data': {'all_data': 'data/newsites.txt', 'train': 'data/train/', 'eval': {'all': 'data/eval/all_eval.txt', 'per_lang': 'data/eval/'}}} 
2024-12-29 22:17:45,256 Training from scratch...
2024-12-29 22:17:46,202 Building model...
2024-12-29 22:17:48,465 Model built with num parameters: 83175286
2024-12-29 22:17:48,465 Building datasets...
2024-12-29 22:17:48,465 Building train dataset from data/train/...
2024-12-29 22:20:43,346 No. of training sentences: 47632
2024-12-29 22:20:43,349 Building evaluation dataset from data/eval/all_eval.txt...
2024-12-29 22:21:02,978 No. of evaluation sentences: 5954
2024-12-29 22:21:02,979 Starting Training...
